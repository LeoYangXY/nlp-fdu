{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf804f9-8d65-47ee-8668-47c0b89ab596",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename assignment-03-###-###.ipynb where ### is your student ID and your name (Chinese).\n",
    "> 2. The deadline of Assignment-03 is 23:59pm, 04-30-2025\n",
    "> 3. Submit a single ZIP archive that includes every file you downloaded. You only need to modify transformer.py, run the Jupyter notebook, and save the resulting performance output inside the archive.\n",
    "> 4. The primary goal of this assignment is to give you hands-on experience implementing a Transformer model.\n",
    "\n",
    "## Task\n",
    "> In this assignment, you will train a Transformer model to count letters. Given a string of characters, your task is to predict, for each position in the string, how many times the character at that position occurred previously, maxing out at 2. This is a 3-class classification task (with labels 0, 1, or > 2, which we’ll just denote as 2). This task is easy with a rule-based system, but it is not so easy for a model to learn. However, Transformers are ideally set up to be able to “look back” with self-attention to count occurrences in the context. Below is an example string (x) (which ends in a trailing space) and its corresponding labels (y):\n",
    "> - x: i like movies a lot\n",
    "> - y: 00010010002102021102\n",
    ">  \n",
    "> If your implementation is correct, then ```python letter_counting.py --task BEFORE```\n",
    "> gives a reasonable output (accuracy will be above 90%).\n",
    "\n",
    "\n",
    "> We also present a modified version of this task that counts both occurrences of letters before and after in the sequence:\n",
    "> - x: i like movies a lot\n",
    "> - y: 22120120102102021102\n",
    ">  \n",
    "> If your implementation is correct, then ```python letter_counting.py --task BEFOREAFTER ``` gives a reasonable output (accuracy will be above 90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7bd2b-8b6f-443f-84b8-3b82f55480b9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "> The dataset for this homework is derived from the text8 collection, which comes from Wikipedia. Your method will use character-level tokenization and operate over text8 sequences that are each exactly 20 characters long. Only 27 character types are present (lowercase characters and spaces); special characters are replaced by a single space and numbers are spelled out as individual digits (50 becomes five zero). Part of examples are:\n",
    "> \n",
    "> - heir average albedo\n",
    "> - ed by rank and file\n",
    "> - s can also extend in\n",
    "> - erages between nine\n",
    "> - that civilization n\n",
    "> - on a t shaped islan\n",
    "> \n",
    "> The dataset is in lettercounting-train.txt and lettercounting-dev.txt. Both two files contain character strings of length 20. You can assume that your model will always see 20 characters as input and make a prediction at each position in the sequence.\n",
    "\n",
    "## Code\n",
    "\n",
    "> The framework code you are given consists of several files.\n",
    "> 1. *utils.py*: it implements an Indexer class, which can be used to maintain a bijective mapping between indices and features (strings).\n",
    "> 2. *letter_counting.py*: contains the driver code, which imports transformer.py, the file you will be editing for this assignment.\n",
    "> 3. *transformer.py*: **You need to fill out all missing parts. Note that your solutions should not use nn.TransformerEncoder, nn.TransformerDecoder, or any other off-the-shelf self-attention layers. You can use nn.Linear, nn.Embedding, and PyTorch’s provided nonlinearities / loss functions to implement Transformers from scratch.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a9711-968f-40b0-92a3-80267839c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python letter_counting.py --task BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246d2e1-17ac-4fc4-8a59-cfc19afc196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda84dc-986f-4e67-847b-9a73464aaeda",
   "metadata": {},
   "source": [
    "Your output ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c17de-eeed-4c8f-91e9-6b8e8ce344f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python letter_counting.py --task BEFOREAFTER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc07e83-1dea-4961-a4e3-245caec33544",
   "metadata": {},
   "source": [
    "Your output ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03415e-6363-49ef-a4d3-7f804376750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python letter_counting.py --task BEFOREAFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73498f-d066-451a-921c-643a122bfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
