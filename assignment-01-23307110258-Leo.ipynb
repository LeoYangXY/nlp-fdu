{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7186371d-a854-4609-8158-8537db05c369",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-01 is 23:59pm, 03-19-2025\n",
    ">\n",
    "> 3. In this assignment, you will\n",
    ">    1) explore Wikipedia text data\n",
    ">    2) build language models\n",
    ">    3) build NB and LR classifiers\n",
    ">\n",
    "> Download the preprocessed data, enwiki-train.json and enwiki-test.json from the Assignment-01 folder. In the data file, each line contains a Wikipedia page with attributes, title, label, and text. There are 9000 records in the train file and 1000 records in test file with ten categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc7cb5-dd64-4886-8cd5-24f147288941",
   "metadata": {},
   "source": [
    "## Task1 - Data exploring\n",
    "\n",
    "> 1) Print out how many documents are in each class  (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ba34ae-48ae-438a-b3dd-c9d4cb3ee416",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import json\n",
    "\n",
    "\n",
    "# 定义文件路径\n",
    "train_path = \"/data/nlp-zbj-22/data/enwiki-train.json\"\n",
    "test_path = \"/data/nlp-zbj-22/data/enwiki-test.json\"\n",
    "\n",
    "# 逐行读取文件,这样才能避免最开始的coding错误（一开始是直接单次loads，就报错了）\n",
    "train_data = []\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "test_data = []\n",
    "with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "# 打印部分内容探查数据\n",
    "# print(\"Train data sample:\", train_data[:1])  \n",
    "# print(\"Test data sample:\", test_data[:1]) \n",
    "\n",
    "train_text=[item[\"text\"] for item in train_data]\n",
    "train_title=[item[\"title\"] for item in train_data]\n",
    "train_label=[item[\"label\"] for item in train_data]\n",
    "\n",
    "test_text = [item[\"text\"] for item in test_data]\n",
    "test_title = [item[\"title\"] for item in test_data]\n",
    "test_label = [item[\"label\"] for item in test_data]\n",
    "\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a2d5c-b719-41ec-8f14-04e092517eb9",
   "metadata": {},
   "source": [
    "> 2) Print out the average number of sentences in each class.\n",
    ">    You may need to use sentence tokenization tools from nltk or spacy.\n",
    ">    (for both train and test dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e85dc7-d50b-406a-9550-b2063f236ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n",
    "import spacy#spacy是默认使用cpu的，我们要显式启用GPU才行(require_gpu是必须使用prefer则是优先使用)\n",
    "#使用pipe进行gpu加速！！！！！！需要处理的文本数据打包成列表，然后通过 nlp.pipe 批量处理这些文本。\n",
    "#nlp.pipe 会将这些文本分成多个批次（根据 batch_size 参数），并将每个批次送入模型进行并行处理。\n",
    "#一开始的问题内存爆了但是GPU的显存没爆，这是因为我们是一次性把数据加载到内存，因此我们换成流式处理\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "from collections import Counter,defaultdict#使用此帮助我们统计类别信息\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WordCount:\n",
    "    def __init__(self, word, count):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.count < other.count  \n",
    "\n",
    "\n",
    "class processor:#架构设计很重要，一定要想清楚每部分功能的依赖顺序，能不能哪个成员函数多写点，另一个成员函数少写点，最后总体完成功能\n",
    "    def __init__(self,label,vocab_size=80000,unk_limit=5):\n",
    "        \n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"#这样子启动显存优化\n",
    "        \n",
    "        self.label=label#不统计数量，是一个list\n",
    "        self.label_num_dict=Counter(label)#统计数量\n",
    "        \n",
    "        # defaultdict 的第一个参数应该是一个工厂函数（如 int, list, set 等），而不是一个列表 label,默认值为 0\n",
    "        self.label_sent_dict = defaultdict(int)  \n",
    "        self.label_word_dict = defaultdict(int)  \n",
    "        for item in label:\n",
    "            self.label_sent_dict[item] = 0\n",
    "            self.label_word_dict[item] = 0   \n",
    "               \n",
    "        self.vocab=None\n",
    "        self.vocab_size = vocab_size  # 词表大小\n",
    "        # 词汇表是纯Python集合对象，因此self.vocab存储在内存中不占用显存\n",
    "        \n",
    "        self.unk_limit = unk_limit  # 低频词阈值\n",
    "        self.unk_token = \"UNK\"  # 低频词标记\n",
    "        self.word_counter=None#这是专门用于preprocess_data这一方法的，单有self.vocab不够\n",
    "        \n",
    "        spacy.require_gpu()\n",
    "\n",
    "        #模型必须要在gpu中才能跑\n",
    "        #每次创建一个实例，都会把模型加载进gpu，这很占用,需要修改代码\n",
    "        #添加模型单例模式，确保所有processor共享同一个模型\n",
    "        if not hasattr(processor, '_shared_model'):\n",
    "            processor._shared_model = spacy.load(\"en_core_web_sm\")\n",
    "        self.my_model = processor._shared_model \n",
    "        \n",
    "        # 新增：定义通用处理参数避免代码重复书写\n",
    "        self.spacy_config = {\n",
    "            \"batch_size\":8\n",
    "        }\n",
    "        self.mem_batch_size = 64 #内存分批大小\n",
    "       \n",
    "    def __del__(self):\n",
    "        \"\"\"析构函数：对象销毁时显式清理显存\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    #流式处理数据，分批生成数据，内存友好\n",
    "    def _batch_generator(self, data):\n",
    "        batch = []\n",
    "        for item in data:\n",
    "            batch.append(item)\n",
    "            if len(batch) >= self.mem_batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                torch.cuda.empty_cache()  # 每批次后清理缓存,避免gpu爆掉\n",
    "        if batch:\n",
    "            yield batch\n",
    "            torch.cuda.empty_cache()  # 处理最后一个小批次后清理\n",
    "        \n",
    "    def count_sentences(self,data):#加载train_data/test_data\n",
    "        self.label_sent_dict.clear()#根据lec2的经验，我们需要每次都清空，因为这是一个实例方法，避免多次调用进行累加\n",
    "        total = len(data)\n",
    "        with tqdm(total=total, desc=\"Counting sentences\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:#用pbar包装进度条\n",
    "            for batch in self._batch_generator(data):\n",
    "                texts = [item[\"text\"] for item in batch]\n",
    "                labels = [item[\"label\"] for item in batch]\n",
    "                # 显存关键点1：及时释放Spacy文档对象\n",
    "                docs = list(self.my_model.pipe(texts, **self.spacy_config))  # 生成文档对象\n",
    "                for doc, label in zip(docs, labels):\n",
    "                        sent_count = len(list(doc.sents))\n",
    "                        self.label_sent_dict[label] += sent_count / self.label_num_dict[label]\n",
    "                        # 显存释放点1：释放文档内部结构\n",
    "                        del doc          # 释放整个文档对象\n",
    "                # 显存释放点2：批量删除中间变量\n",
    "                del texts, labels, docs  # 解除所有引用\n",
    "                torch.cuda.empty_cache()  # 立即清空缓存（但是这会造成中断，太频繁了可能会造成过大的系统开销）\n",
    "                pbar.update(len(batch)) \n",
    "        return self.label_sent_dict    \n",
    "        \n",
    "    def count_words(self,data):#加载train_data/test_data\n",
    "        self.label_word_dict.clear()\n",
    "        total = len(data)\n",
    "        with tqdm(total=total, desc=\"Counting words\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:\n",
    "            for batch in self._batch_generator(data):\n",
    "                texts = [item[\"text\"] for item in batch]\n",
    "                labels = [item[\"label\"] for item in batch]\n",
    "                \n",
    "                docs = list(self.my_model.pipe(texts, **self.spacy_config))  # 显式转换为列表，方便后续释放显存\n",
    "                for doc, label in zip(docs, labels):\n",
    "                    word_count = len([t for t in doc if not t.is_punct and not t.is_space])\n",
    "                    self.label_word_dict[label] += word_count / self.label_num_dict[label]\n",
    "                    \n",
    "                    # 释放文档对象（之前缺失）\n",
    "                    del doc\n",
    "                \n",
    "                # 新增批量释放\n",
    "                del texts, labels, docs\n",
    "                torch.cuda.empty_cache()  # 新增\n",
    "                \n",
    "                pbar.update(len(batch))\n",
    "        return self.label_word_dict\n",
    "                 \n",
    "    def build_vocab(self,data):\n",
    "    # 构建词表和预处理文本的顺序取决于你的具体需求和数据特点,不一定非得谁在前。\n",
    "    # 此处我们的架构是必须先调用build_vocab，再调用preprocess和first_forty\n",
    "    # 构建词表，仅受固定大小的约束，但是我们不特别处理低频词\n",
    "        self.word_counter = Counter()\n",
    "        total = len(data)\n",
    "        with tqdm(total=total, desc=\"Building vocabulary\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:\n",
    "            for batch in self._batch_generator(data):\n",
    "                texts = [item[\"text\"] for item in batch]\n",
    "                docs = list(self.my_model.pipe(texts, **self.spacy_config))\n",
    "                for doc in docs:\n",
    "                    words = [token.text.lower() for token in doc\n",
    "                            if not token.is_punct\n",
    "                            and not token.is_space\n",
    "                            and not token.is_stop\n",
    "                            and not token.is_digit\n",
    "                            and not token.is_currency\n",
    "                            and token.text != '<'\n",
    "                            and token.text != '>']\n",
    "                    self.word_counter.update(words)\n",
    "                    del doc\n",
    "                # 新增批量释放（释放这一批次的docs，texts的显存）\n",
    "                del texts, docs\n",
    "                torch.cuda.empty_cache()  # 新增\n",
    "                pbar.update(len(batch)) \n",
    "        self.vocab = {word for word, freq in self.word_counter.most_common(self.vocab_size)}\n",
    "        self.vocab.add(self.unk_token)\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        # 预处理文本，并替换低频词和不在词表中的词为UNK\n",
    "        if self.word_counter is None:\n",
    "            raise ValueError(\"Vocabulary has not been built. Call build_vocab first.\")\n",
    "    \n",
    "        processed_data = []\n",
    "        total = len(data)\n",
    "        with tqdm(total=total, desc=\"Preprocessing data\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:\n",
    "            for batch in self._batch_generator(data):\n",
    "                # 从batch中同时提取text和对应的label\n",
    "                texts = [item[\"text\"] for item in batch]\n",
    "                labels = [item[\"label\"] for item in batch]  # 提取标签\n",
    "                docs = list(self.my_model.pipe(texts, **self.spacy_config)) #把texts这个字符串对象处理为spacy的内置对象\n",
    "                for item_label, doc in zip(labels, docs):\n",
    "                    words = [\n",
    "                        token.text.lower() \n",
    "                        for token in doc\n",
    "                        if not token.is_punct\n",
    "                        and not token.is_space\n",
    "                        and not token.is_stop\n",
    "                        and not token.is_digit\n",
    "                        and not token.is_currency\n",
    "                        and token.text not in ('<', '>')\n",
    "                    ]\n",
    "                    \n",
    "                    # 替换低频词为UNK\n",
    "                    words = [\n",
    "                        word if (word in self.vocab and self.word_counter[word] >= self.unk_limit)\n",
    "                        else self.unk_token\n",
    "                        for word in words\n",
    "                    ]\n",
    "                    \n",
    "                    # 将处理后的数据添加到结果集\n",
    "                    processed_data.append({\n",
    "                        \"label\": item_label,  # 使用从数据项中提取的标签\n",
    "                        \"text\": \" \".join(words)\n",
    "                    })\n",
    "                    \n",
    "                    # 显存释放点3：预处理完成后立即释放\n",
    "                    del doc\n",
    "                \n",
    "                # 显存释放点4：清除批次变量\n",
    "                del texts, labels, docs\n",
    "                torch.cuda.empty_cache()  # 每批处理后清理\n",
    "                pbar.update(len(batch))\n",
    "        \n",
    "        return processed_data\n",
    "\n",
    "    def first_forty(self, data):\n",
    "        first_articles = {}\n",
    "        remaining_labels = set(self.label_num_dict.keys())\n",
    "        total = len(remaining_labels)  # 进度条总数为类别数量\n",
    "        with tqdm(total=total, desc=\"Finding first forty words\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:  \n",
    "            for batch in self._batch_generator(data):\n",
    "                # 提前退出检查点1\n",
    "                if not remaining_labels:\n",
    "                    break  #直接跳出外层循环                \n",
    "                texts = [item[\"text\"] for item in batch]\n",
    "                titles = [item[\"title\"] for item in batch]\n",
    "                labels = [item[\"label\"] for item in batch]\n",
    "                docs = list(self.my_model.pipe(texts, **self.spacy_config))\n",
    "                for doc, label, title in zip(docs, labels, titles):\n",
    "                    # 提前退出检查点2\n",
    "                    if not remaining_labels:\n",
    "                        break  #跳出内层循环\n",
    "                    if label not in first_articles and label in remaining_labels:\n",
    "                        count_dict=defaultdict(int)\n",
    "                        for token in doc:\n",
    "                            if (not token.is_punct and not token.is_space and not token.is_stop and not token.is_digit  and not token.is_currency and token.text != '<' and token.text !='>'):\n",
    "                                word=token.text.lower()\n",
    "                                count_dict[word]+=1 \n",
    "                        heap = []\n",
    "                        for word, count in count_dict.items():\n",
    "                            if len(heap) < 40:\n",
    "                                heapq.heappush(heap, WordCount(word, count))\n",
    "                            else:\n",
    "                                if count > heap[0].count:\n",
    "                                    heapq.heappop(heap)  \n",
    "                                    heapq.heappush(heap, WordCount(word, count))     \n",
    "                        del doc#这里是减少引用，还没真正回收。后面一个batch完成的时候统一回收\n",
    "                        \n",
    "                        #如果是最开始重写__lt__方法为>号，那么就是构建的按照count降序的heap那么每次的比较逻辑应该是\n",
    "                        #如果当前实例比堆尾的大，那就弹出堆尾的，然后heappush；如果没有堆尾大，那就下一个（注意heappop是弹出堆顶的实例）\n",
    "                        top_words = [wc.word for wc in heap]\n",
    "                        top_words = top_words[::-1]  # 从高到低排序\n",
    "                        first_articles[label] = (title, top_words)\n",
    "                        remaining_labels.remove(label)\n",
    "                        pbar.update(1)  # 每处理一个类别更新 1 次 \n",
    "                        \n",
    "                #释放这一个batch的临时变量\n",
    "                #每次del variable会将该变量名对应的引用计数减1,但是要使用empty_cache（回收引用=0的变量占的显存）才是真正的回收\n",
    "                del texts, titles, labels, docs \n",
    "                torch.cuda.empty_cache() \n",
    "        return [{\n",
    "            \"Class\": label,\n",
    "            \"First article's name\": first_articles[label][0],\n",
    "            \"Processed words\": ' '.join(first_articles[label][1])\n",
    "        } for label in self.label_num_dict.keys() if label in first_articles]  \n",
    "\n",
    "\n",
    "# # 初次加载得到预处理的结果\n",
    "# # 处理训练数据\n",
    "# p_train = processor(train_label)\n",
    "# p_train.build_vocab(train_data)\n",
    "# pre_data_train = p_train.preprocess_data(train_data)\n",
    "# first_forty_train = p_train.first_forty(train_data)\n",
    "# sent_train = p_train.count_sentences(train_data)\n",
    "# word_train = p_train.count_words(train_data)\n",
    "\n",
    "\n",
    "\n",
    "# # 处理测试数据\n",
    "# p_test = processor(test_label)\n",
    "# p_test.vocab = p_train.vocab  # 共享训练数据的词表\n",
    "# p_test.word_counter = p_train.word_counter  # 共享训练数据的词频统计\n",
    "# pre_data_test = p_test.preprocess_data(test_data)\n",
    "# first_forty_test = p_test.first_forty(test_data)\n",
    "# sent_test = p_test.count_sentences(test_data)\n",
    "# word_test = p_test.count_words(test_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d082c0ed-7a29-4f36-8197-3ff1cc7dca68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Book': 4602.229603729603, 'Food': 2925.3966942148754, 'Film': 3827.0879360465124, 'Politician': 5029.373147340897, 'Animal': 1232.7682926829268, 'Writer': 4892.925877763328, 'Artist': 4147.875273522975, 'Disease': 7096.128712871285, 'Actor': 1400.3544303797469, 'Software': 4227.548117154812}\n",
      "{'Politician': 5248.6736292428195, 'Writer': 5006.0, 'Book': 4441.5641025641025, 'Film': 3745.638513513515, 'Artist': 3899.142857142858, 'Actor': 3624.0, 'Food': 2974.8125, 'Disease': 6946.499999999999, 'Software': 4144.148148148149, 'Animal': 1122.0}\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "save_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "\n",
    "\n",
    "word_train = json.load(open(os.path.join(save_dir, \"train_word_counts.json\")))\n",
    "word_test = json.load(open(os.path.join(save_dir, \"test_word_counts.json\")))\n",
    "print(word_train)\n",
    "print(word_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf90f098-7b77-4c43-9824-3cbea57fad94",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data:  34%|███▍      | 3072/9000 [18:45<36:12,  2.73it/s,  34%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 339\u001b[0m\n\u001b[1;32m    337\u001b[0m p_experiment\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m xsm\u001b[38;5;241m.\u001b[39mvocab  \u001b[38;5;66;03m# 共享训练数据的词表\u001b[39;00m\n\u001b[1;32m    338\u001b[0m p_experiment\u001b[38;5;241m.\u001b[39mword_counter \u001b[38;5;241m=\u001b[39m xsm\u001b[38;5;241m.\u001b[39mword_counter  \u001b[38;5;66;03m# 共享训练数据的词频统计\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m p_experiment_pre_data\u001b[38;5;241m=\u001b[39m\u001b[43mp_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m p_experiment_count_words\u001b[38;5;241m=\u001b[39mp_experiment\u001b[38;5;241m.\u001b[39mcount_words(train_data)\n",
      "Cell \u001b[0;32mIn[3], line 246\u001b[0m, in \u001b[0;36mprocessor.preprocess_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mtotal, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing data\u001b[39m\u001b[38;5;124m\"\u001b[39m, bar_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[38;5;124m| \u001b[39m\u001b[38;5;132;01m{n_fmt}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{total_fmt}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{elapsed}\u001b[39;00m\u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{remaining}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{rate_fmt}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{percentage:3.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_generator(data):\n\u001b[1;32m    245\u001b[0m          \u001b[38;5;66;03m# 批量转换为索引张量\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m         batch_tensors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_to_indices(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# 使用PyTorch进行向量化处理\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch_tensors], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 246\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mtotal, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing data\u001b[39m\u001b[38;5;124m\"\u001b[39m, bar_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[38;5;124m| \u001b[39m\u001b[38;5;132;01m{n_fmt}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{total_fmt}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{elapsed}\u001b[39;00m\u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{remaining}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{rate_fmt}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{percentage:3.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_generator(data):\n\u001b[1;32m    245\u001b[0m          \u001b[38;5;66;03m# 批量转换为索引张量\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m         batch_tensors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_to_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# 使用PyTorch进行向量化处理\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch_tensors], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 102\u001b[0m, in \u001b[0;36mprocessor._text_to_indices\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_text_to_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"将文本转换为索引张量\"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     words \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmy_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_filter(token)]\n\u001b[1;32m    103\u001b[0m     indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_idx\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_idx) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(indices, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/spacy/language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/spacy/ml/tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/spacy/ml/parser_model.pyx:257\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/spacy/ml/parser_model.pyx:407\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/thinc/model.py:230\u001b[0m, in \u001b[0;36mModel.get_param\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_param\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FloatsXd:\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve a weights parameter by name.\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_params:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # # experiment-version\n",
    "# # # experiment-version\n",
    "# # # experiment-version\n",
    "# # # experiment-version\n",
    "# # # experiment-version\n",
    "# # # experiment-version\n",
    "# # # experiment-version\n",
    "\n",
    "\n",
    "# # Your code goes to here\n",
    "\n",
    "# import spacy#spacy是默认使用cpu的，我们要显式启用GPU才行(require_gpu是必须使用prefer则是优先使用)\n",
    "# #使用pipe进行gpu加速！！！！！！需要处理的文本数据打包成列表，然后通过 nlp.pipe 批量处理这些文本。\n",
    "# #nlp.pipe 会将这些文本分成多个批次（根据 batch_size 参数），并将每个批次送入模型进行并行处理。\n",
    "# #一开始的问题内存爆了但是GPU的显存没爆，这是因为我们是一次性把数据加载到内存，因此我们换成流式处理\n",
    "\n",
    "# import torch\n",
    "# import re\n",
    "# import os\n",
    "# from collections import Counter,defaultdict#使用此帮助我们统计类别信息\n",
    "# import heapq\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# class WordCount:\n",
    "#     def __init__(self, word, count):\n",
    "#         self.word = word\n",
    "#         self.count = count\n",
    "\n",
    "#     def __lt__(self, other):\n",
    "#         return self.count < other.count  \n",
    "\n",
    "\n",
    "# class processor:#架构设计很重要，一定要想清楚每部分功能的依赖顺序，能不能哪个成员函数多写点，另一个成员函数少写点，最后总体完成功能\n",
    "#     def __init__(self,label,vocab_size=20000,unk_limit=10):\n",
    "        \n",
    "#         os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"#这样子启动显存优化\n",
    "        \n",
    "#         self.label=label#不统计数量，是一个list\n",
    "#         self.label_num_dict=Counter(label)#统计数量\n",
    "        \n",
    "#         # defaultdict 的第一个参数应该是一个工厂函数（如 int, list, set 等），而不是一个列表 label,默认值为 0\n",
    "#         self.label_sent_dict = defaultdict(int)  \n",
    "#         self.label_word_dict = defaultdict(int)  \n",
    "#         for item in label:\n",
    "#             self.label_sent_dict[item] = 0\n",
    "#             self.label_word_dict[item] = 0   \n",
    "               \n",
    "#         self.vocab=None\n",
    "#         self.vocab_size = vocab_size  # 词表大小\n",
    "#         # 词汇表是纯Python集合对象，因此self.vocab存储在内存中不占用显存\n",
    "        \n",
    "#         self.unk_limit = unk_limit  # 低频词阈值\n",
    "#         self.unk_token = \"UNK\"  # 低频词标记\n",
    "#         self.word_counter=None#这是专门用于preprocess_data这一方法的，单有self.vocab不够\n",
    "        \n",
    "#         spacy.require_gpu()\n",
    "\n",
    "        \n",
    "#         # 新增映射表：在构建词表的时候所有操作不变,慢点就慢点,但是之后的所有操作,都把单词通过词表映射为数字,这样子进行向量化处理\n",
    "#         self.word_to_idx = {}  # 词到索引的映射\n",
    "#         self.idx_to_word = {}  # 索引到词的映射\n",
    "#         self.unk_idx = 0       # UNK的索引位置\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#         #模型必须要在gpu中才能跑\n",
    "#         #每次创建一个实例，都会把模型加载进gpu，这很占用,需要修改代码\n",
    "#         #添加模型单例模式，确保所有processor共享同一个模型\n",
    "#         if not hasattr(processor, '_shared_model'):\n",
    "#             processor._shared_model = spacy.load(\"en_core_web_sm\")\n",
    "#         self.my_model = processor._shared_model \n",
    "        \n",
    "#         # 新增：定义通用处理参数避免代码重复书写\n",
    "#         self.spacy_config = {\n",
    "#             \"batch_size\":512\n",
    "#         }\n",
    "#         self.mem_batch_size = 1024#内存分批大小\n",
    "       \n",
    "#     def __del__(self):\n",
    "#         \"\"\"析构函数：对象销毁时显式清理显存\"\"\"\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#     #流式处理数据，分批生成数据，内存友好\n",
    "#     def _batch_generator(self, data):\n",
    "#         batch = []\n",
    "#         for item in data:\n",
    "#             batch.append(item)\n",
    "#             if len(batch) >= self.mem_batch_size:\n",
    "#                 yield batch\n",
    "#                 batch = []\n",
    "#                 torch.cuda.empty_cache()  # 每批次后清理缓存,避免gpu爆掉\n",
    "#         if batch:\n",
    "#             yield batch\n",
    "#             torch.cuda.empty_cache()  # 处理最后一个小批次后清理\n",
    "\n",
    "\n",
    "#     def _text_to_indices(self, text):\n",
    "#         \"\"\"将文本转换为索引张量\"\"\"\n",
    "#         words = [token.text.lower() for token in self.my_model(text) if self._token_filter(token)]\n",
    "#         indices = [self.word_to_idx.get(word, self.unk_idx) for word in words]\n",
    "#         return torch.tensor(indices, dtype=torch.long, device='cuda')\n",
    "    \n",
    "#     def _token_filter(self, token):\n",
    "#         return (not token.is_punct and not token.is_space and not token.is_stop \n",
    "#                 and not token.is_digit and not token.is_currency \n",
    "#                 and token.text not in ('<', '>'))\n",
    "    \n",
    "#     def count_sentences(self, data):\n",
    "#         self.label_sent_dict.clear()\n",
    "#         total = len(data)\n",
    "        \n",
    "#         # 使用索引进行统计\n",
    "#         label_tensors = []\n",
    "#         sent_count_tensors = []\n",
    "        \n",
    "#         with tqdm(total=total, desc=\"Counting sentences\", \n",
    "#                 bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:\n",
    "#             for batch in self._batch_generator(data):\n",
    "#                 texts = [item[\"text\"] for item in batch]\n",
    "#                 labels = [item[\"label\"] for item in batch]\n",
    "                \n",
    "#                 # 生成Spacy文档对象\n",
    "#                 docs = list(self.my_model.pipe(texts, **self.spacy_config))\n",
    "                \n",
    "#                 # 获取句子数和标签索引\n",
    "#                 batch_sent_counts = [len(list(doc.sents)) for doc in docs]\n",
    "#                 try:\n",
    "#                     batch_labels = [self.label.index(l) for l in labels]\n",
    "#                 except ValueError as e:\n",
    "#                     print(f\"发现无效标签: {e}\")\n",
    "#                     batch_labels = []\n",
    "                \n",
    "#                 # 转换为张量\n",
    "#                 if batch_labels:\n",
    "#                     label_tensors.append(torch.tensor(batch_labels, device='cuda'))\n",
    "#                     sent_count_tensors.append(torch.tensor(batch_sent_counts, dtype=torch.float32, device='cuda'))\n",
    "                \n",
    "#                 # 显存释放优化\n",
    "#                 del texts, labels, docs, batch_sent_counts\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 pbar.update(len(batch))\n",
    "        \n",
    "#         # 聚合统计结果\n",
    "#         if label_tensors and sent_count_tensors:\n",
    "#             all_labels = torch.cat(label_tensors)\n",
    "#             all_counts = torch.cat(sent_count_tensors)\n",
    "#             result = torch.zeros(len(self.label), device='cuda')\n",
    "#             result.scatter_add_(0, all_labels, all_counts)\n",
    "#         else:\n",
    "#             result = torch.zeros(len(self.label), device='cuda')\n",
    "        \n",
    "#         # 转换为原有格式\n",
    "#         for idx, total in enumerate(result.cpu().numpy()):\n",
    "#             label = self.label[idx]\n",
    "#             if self.label_num_dict[label] > 0:  # 防止除零错误\n",
    "#                 self.label_sent_dict[label] = total / self.label_num_dict[label]\n",
    "        \n",
    "#         return self.label_sent_dict  \n",
    "        \n",
    "#     def count_words(self,data):#加载train_data/test_data\n",
    "#         self.label_word_dict.clear()\n",
    "        \n",
    "#         # 使用索引进行统计\n",
    "#         label_tensors = []\n",
    "#         word_count_tensors = []\n",
    "        \n",
    "#         with tqdm(total=total, desc=\"Counting words\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:\n",
    "#             for batch in self._batch_generator(data):\n",
    "                \n",
    "#                 # 生成索引张量\n",
    "#                 batch_indices = [self._text_to_indices(item[\"text\"]) for item in batch]\n",
    "#                 batch_labels = [self.label.index(item[\"label\"]) for item in batch]\n",
    "                \n",
    "#                 # 统计非UNK词数\n",
    "#                 valid_counts = torch.stack([(t != self.unk_idx).sum() for t in batch_indices])\n",
    "                \n",
    "#                 # 收集数据\n",
    "#                 label_tensors.append(torch.tensor(batch_labels, device='cuda'))\n",
    "#                 word_count_tensors.append(valid_counts.float())\n",
    "                \n",
    "#                 del batch_indices\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 pbar.update(len(batch))\n",
    "        \n",
    "#         # 聚合统计结果\n",
    "#         all_labels = torch.cat(label_tensors)\n",
    "#         all_counts = torch.cat(word_count_tensors)\n",
    "#         result = torch.zeros(len(self.label), device='cuda')\n",
    "#         result.scatter_add_(0, all_labels, all_counts)\n",
    "        \n",
    "#         # 转换为原有格式\n",
    "#         for idx, count in enumerate(result.cpu().numpy()):\n",
    "#             label = self.label[idx]\n",
    "#             self.label_word_dict[label] = count / self.label_num_dict[label]\n",
    "        \n",
    "#         return self.label_word_dict\n",
    "                 \n",
    "#     def build_vocab(self,data):\n",
    "#     # 构建词表和预处理文本的顺序取决于你的具体需求和数据特点,不一定非得谁在前。\n",
    "#     # 此处我们的架构是必须先调用build_vocab，再调用preprocess和first_forty\n",
    "#     # 构建词表，仅受固定大小的约束，但是我们不特别处理低频词\n",
    "#         self.word_counter = Counter()\n",
    "#         total = len(data)\n",
    "#         with tqdm(total=total, desc=\"Building vocabulary\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:\n",
    "#             for batch in self._batch_generator(data):\n",
    "#                 texts = [item[\"text\"] for item in batch]\n",
    "#                 docs = list(self.my_model.pipe(texts, **self.spacy_config))\n",
    "#                 for doc in docs:\n",
    "#                     words = [token.text.lower() for token in doc\n",
    "#                             if not token.is_punct\n",
    "#                             and not token.is_space\n",
    "#                             and not token.is_stop\n",
    "#                             and not token.is_digit\n",
    "#                             and not token.is_currency\n",
    "#                             and token.text != '<'\n",
    "#                             and token.text != '>']\n",
    "#                     self.word_counter.update(words)\n",
    "#                     del doc\n",
    "#                 # 新增批量释放（释放这一批次的docs，texts的显存）\n",
    "#                 del texts, docs\n",
    "#                 torch.cuda.empty_cache()  # 新增\n",
    "#                 pbar.update(len(batch)) \n",
    "#         self.vocab = {word for word, freq in self.word_counter.most_common(self.vocab_size)}\n",
    "#         self.vocab.add(self.unk_token)\n",
    "        \n",
    "#         # 构建映射表\n",
    "#         self.word_to_idx = {word: idx+1 for idx, word in enumerate(self.vocab)}  # 索引从1开始\n",
    "#         self.word_to_idx[self.unk_token] = 0\n",
    "#         self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
    "#         self.unk_idx = self.word_to_idx[self.unk_token]\n",
    "\n",
    "    \n",
    "#     def preprocess_data(self, data):\n",
    "#         # 预处理文本，并替换低频词和不在词表中的词为UNK\n",
    "#         if self.word_counter is None:\n",
    "#             raise ValueError(\"Vocabulary has not been built. Call build_vocab first.\")\n",
    "    \n",
    "#         processed_data = []\n",
    "#         total = len(data)\n",
    "#         with tqdm(total=total, desc=\"Preprocessing data\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:\n",
    "#             for batch in self._batch_generator(data):\n",
    "#                  # 批量转换为索引张量\n",
    "#                 batch_tensors = [self._text_to_indices(item[\"text\"]) for item in batch]\n",
    "                                \n",
    "#                 # 使用PyTorch进行向量化处理\n",
    "#                 lengths = torch.tensor([len(t) for t in batch_tensors], device='cuda')\n",
    "#                 padded = torch.nn.utils.rnn.pad_sequence(batch_tensors, batch_first=True)\n",
    "                \n",
    "#                 # 生成有效词掩码 (统计时使用)\n",
    "#                 valid_mask = (padded != self.unk_idx)\n",
    "                \n",
    "#                 # 转换为文本的优化版本\n",
    "#                 for i in range(len(batch)):\n",
    "#                     valid_indices = padded[i][valid_mask[i]]\n",
    "#                     processed_text = ' '.join([self.idx_to_word[idx.item()] for idx in valid_indices])\n",
    "#                     processed_data.append({\n",
    "#                         \"label\": batch[i][\"label\"],\n",
    "#                         \"text\": processed_text\n",
    "#                     })\n",
    "                    \n",
    "#                 # 显存清理\n",
    "#                 del batch_tensors, padded, valid_mask\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 pbar.update(len(batch))\n",
    "                \n",
    "#         return processed_data\n",
    "\n",
    "#     def first_forty(self, data):\n",
    "#         first_articles = {}\n",
    "#         remaining_labels = set(self.label_num_dict.keys())\n",
    "#         total = len(remaining_labels)  # 进度条总数为类别数量\n",
    "#         with tqdm(total=total, desc=\"Finding first forty words\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {percentage:3.0f}%]\") as pbar:  \n",
    "#             for batch in self._batch_generator(data):\n",
    "#                 # 提前退出检查点1\n",
    "#                 if not remaining_labels:\n",
    "#                     break  #直接跳出外层循环                \n",
    "#                 texts = [item[\"text\"] for item in batch]\n",
    "#                 titles = [item[\"title\"] for item in batch]\n",
    "#                 labels = [item[\"label\"] for item in batch]\n",
    "#                 docs = list(self.my_model.pipe(texts, **self.spacy_config))\n",
    "#                 for doc, label, title in zip(docs, labels, titles):\n",
    "#                     # 提前退出检查点2\n",
    "#                     if not remaining_labels:\n",
    "#                         break  #跳出内层循环\n",
    "#                     if label not in first_articles and label in remaining_labels:\n",
    "#                         count_dict=defaultdict(int)\n",
    "#                         for token in doc:\n",
    "#                             if (not token.is_punct and not token.is_space and not token.is_stop and not token.is_digit  and not token.is_currency and token.text != '<' and token.text !='>'):\n",
    "#                                 word=token.text.lower()\n",
    "#                                 count_dict[word]+=1 \n",
    "#                         heap = []\n",
    "#                         for word, count in count_dict.items():\n",
    "#                             if len(heap) < 40:\n",
    "#                                 heapq.heappush(heap, WordCount(word, count))\n",
    "#                             else:\n",
    "#                                 if count > heap[0].count:\n",
    "#                                     heapq.heappop(heap)  \n",
    "#                                     heapq.heappush(heap, WordCount(word, count))     \n",
    "#                         del doc#这里是减少引用，还没真正回收。后面一个batch完成的时候统一回收\n",
    "                        \n",
    "#                         #如果是最开始重写__lt__方法为>号，那么就是构建的按照count降序的heap那么每次的比较逻辑应该是\n",
    "#                         #如果当前实例比堆尾的大，那就弹出堆尾的，然后heappush；如果没有堆尾大，那就下一个（注意heappop是弹出堆顶的实例）\n",
    "#                         top_words = [wc.word for wc in heap]\n",
    "#                         top_words = top_words[::-1]  # 从高到低排序\n",
    "#                         first_articles[label] = (title, top_words)\n",
    "#                         remaining_labels.remove(label)\n",
    "#                         pbar.update(1)  # 每处理一个类别更新 1 次 \n",
    "                        \n",
    "#                 #释放这一个batch的临时变量\n",
    "#                 #每次del variable会将该变量名对应的引用计数减1,但是要使用empty_cache（回收引用=0的变量占的显存）才是真正的回收\n",
    "#                 del texts, titles, labels, docs \n",
    "#                 torch.cuda.empty_cache() \n",
    "#         return [{\n",
    "#             \"Class\": label,\n",
    "#             \"First article's name\": first_articles[label][0],\n",
    "#             \"Processed words\": ' '.join(first_articles[label][1])\n",
    "#         } for label in self.label if label in first_articles]\n",
    "\n",
    "# import os\n",
    "# import pickle\n",
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# # 加载路径设置\n",
    "# current_dir = os.path.abspath(\"\")\n",
    "# parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "# save_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "\n",
    "# with open(os.path.join(save_dir, \"train_processor.pkl\"), \"rb\") as f:\n",
    "#     xsm= pickle.load(f)\n",
    "\n",
    "# # 处理训练数据-experiment\n",
    "# p_experiment=processor(train_label)\n",
    "# p_experiment.vocab = xsm.vocab  # 共享训练数据的词表\n",
    "# p_experiment.word_counter = xsm.word_counter  # 共享训练数据的词频统计\n",
    "# p_experiment_pre_data=p_experiment.preprocess_data(train_data)\n",
    "# p_experiment_count_words=p_experiment.count_words(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf06365-6b50-4b4a-a86b-2a5cf5f0b80c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录：/data/nlp-zbj-22/nlp-fdu\n",
      "父级目录：/data/nlp-zbj-22\n",
      "保存目录：/data/nlp-zbj-22/processed_data\n",
      "所有数据已保存到：/data/nlp-zbj-22/processed_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 获取当前笔记本的绝对路径\n",
    "current_dir = os.path.abspath(\"\")  # 获取nlp-fdu文件夹的路径\n",
    "print(f\"当前工作目录：{current_dir}\")\n",
    "\n",
    "# 构建父目录路径（nlp-zbj-22）\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))  # 向上跳一级目录\n",
    "print(f\"父级目录：{parent_dir}\")\n",
    "\n",
    "# 创建目标保存目录\n",
    "save_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"保存目录：{save_dir}\")\n",
    "\n",
    "# # ===== 训练数据保存 =====\n",
    "# # 保存处理器对象\n",
    "# with open(os.path.join(save_dir, \"train_processor.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(p_train, f)\n",
    "\n",
    "# # 保存预处理数据（numpy格式）\n",
    "# np.save(os.path.join(save_dir, \"train_preprocessed.npy\"), pre_data_train)\n",
    "\n",
    "# 保存first_forty结果\n",
    "with open(os.path.join(save_dir, \"train_first40.json\"), \"w\") as f:\n",
    "    json.dump(first_forty_train, f)\n",
    "\n",
    "# # 分别保存句子和单词统计（独立文件）\n",
    "# with open(os.path.join(save_dir, \"train_sentence_counts.json\"), \"w\") as f:\n",
    "#     json.dump(sent_train, f)\n",
    "\n",
    "# with open(os.path.join(save_dir, \"train_word_counts.json\"), \"w\") as f:\n",
    "#     json.dump(word_train, f)\n",
    "\n",
    "# # ===== 测试数据保存 =====\n",
    "# # 保存测试处理器\n",
    "# with open(os.path.join(save_dir, \"test_processor.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(p_test, f)\n",
    "\n",
    "# # 保存测试预处理数据\n",
    "# np.save(os.path.join(save_dir, \"test_preprocessed.npy\"), pre_data_test)\n",
    "\n",
    "# 保存测试first_forty\n",
    "with open(os.path.join(save_dir, \"test_first40.json\"), \"w\") as f:\n",
    "    json.dump(first_forty_test, f)\n",
    "\n",
    "# # 分别保存测试统计\n",
    "# with open(os.path.join(save_dir, \"test_sentence_counts.json\"), \"w\") as f:\n",
    "#     json.dump(sent_test, f)\n",
    "\n",
    "# with open(os.path.join(save_dir, \"test_word_counts.json\"), \"w\") as f:\n",
    "#     json.dump(word_test, f)\n",
    "\n",
    "print(f\"所有数据已保存到：{save_dir}\")\n",
    "\n",
    "# 上面的代码把训练好的数据进行保存，然后需要调用的时候我们使用下面的代码：\n",
    "\n",
    "# # 加载路径设置\n",
    "# current_dir = os.path.abspath(\"\")\n",
    "# parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "# save_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "\n",
    "# # 加载训练数据示例\n",
    "# with open(os.path.join(save_dir, \"train_processor.pkl\"), \"rb\") as f:\n",
    "#     p_train = pickle.load(f)\n",
    "# 加载词表（通过训练处理器）\n",
    "# vocab = p_train.vocab  # 假设处理器有vocab属性\n",
    "# print(f\"词表大小：{len(vocab)} | 示例：{list(vocab.items())[:5]}\")\n",
    "\n",
    "# # 加载预处理后的训练数据\n",
    "# preprocessed_train = np.load(os.path.join(save_dir, \"train_preprocessed.npy\"))\n",
    "# print(f\"训练数据形状：{preprocessed_train.shape} | 数据类型：{preprocessed_train.dtype}\")\n",
    "# sent_train = json.load(open(os.path.join(save_dir, \"train_sentence_counts.json\")))\n",
    "# word_train = json.load(open(os.path.join(save_dir, \"train_word_counts.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ef409-65a5-4d07-9b69-c5986569970a",
   "metadata": {},
   "source": [
    "> 3) Print out the average number of tokens in each class\n",
    ">    (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d7628e-762c-49fe-804a-796fa0265af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Book': 204.6585081585083, 'Food': 149.67768595041326, 'Film': 177.17478197674413, 'Politician': 222.32374309793562, 'Animal': 66.2439024390244, 'Writer': 216.60728218465502, 'Artist': 185.88402625820552, 'Disease': 344.05940594059405, 'Actor': 68.60759493670886, 'Software': 199.07949790794976}\n",
      "{'Politician': 230.63185378590083, 'Writer': 222.07352941176464, 'Book': 197.3846153846154, 'Film': 173.3986486486487, 'Artist': 174.90476190476184, 'Actor': 175.0, 'Food': 162.0, 'Disease': 323.22222222222223, 'Software': 202.22222222222229, 'Animal': 61.63636363636363}\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "sent_train = json.load(open(os.path.join(save_dir, \"train_sentence_counts.json\")))\n",
    "sent_test = json.load(open(os.path.join(save_dir, \"test_sentence_counts.json\")))\n",
    "print(sent_train)\n",
    "print(sent_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999a66-977c-469b-939c-c3934210972e",
   "metadata": {},
   "source": [
    "> 4) For each sentence in the document, remove punctuations and other special characters so that each sentence only contains English words and numbers. To make your life easier, you can make all words as lower cases. For each class, print out the first article's name and the processed first 40 words. (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "819a28ef-0821-4ace-be70-fb9b249d355b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Class': 'Book', \"First article's name\": 'Middlesex_(novel)', 'Processed words': 'lefty cal callie male book americans desdemona person middlesex family story identity eugenides intersex years writing female novel greek life noted american gender events hermaphrodite people new voice wrote grandparents narrative detroit york characters considered incest according man relationship condition'}, {'Class': 'Food', \"First article's name\": 'Chowder', 'Processed words': 'smoked potato zealand french stew salmon seafood clams chowder prepared clam england new dish style fish soup corn haddock chowders marinara american tomatoes potatoes fresh ingredients cream milk popular called canned instead pork onion tomato crackers mixed word use salt'}, {'Class': 'Film', \"First article's name\": 'Young_People_Fucking', 'Processed words': 'rated bill abrams canada sex release canadian wrote people fucking characters said gero toronto best new young found film title maple controversy cast relationship sexual films gord ken time c-10 dave production critics think called archetype publicity filming male inez'}, {'Class': 'Politician', \"First article's name\": 'Miklós_Horthy', 'Processed words': 'prime hitler war troops hungarian german world state horthy jewish hungary soviet jews communist government budapest germany nazi minister army later germans allies national cross anti june regent miklós country prónay began arrow party trianon forces axis march terror known'}, {'Class': 'Animal', \"First article's name\": 'Lunaspis', 'Processed words': 'mudstone median dorsal trunk fish plates like heroldi lunaspis broilii l. species specimens specimen australia anterior genus found shield long tubercles according elongated moon prumiensis discovered new broili germany marine near bony crescent waitahu gross shape plate armor body head'}, {'Class': 'Writer', \"First article's name\": 'Maria_Valtorta', 'Processed words': 'supernatural holy man work poem wrote office pope valtorta viareggio index bishop italian life father church maria letter book jesus faithful ratzinger god reported permission visions years cardinal received congregation publication danylak doctrine published priest said faith notebooks migliorini called'}, {'Class': 'Artist', \"First article's name\": 'Dan_Graham', 'Processed words': 'museum pavilions conceptual art performance video audience space graham new film dan way glass work mirror works pavilion religion series viewer rock time gallery york changes piece early photographs magazine man sculpture public inside artist american lewitt project exhibited started'}, {'Class': 'Disease', \"First article's name\": 'Down_syndrome', 'Processed words': 'therapy screening cases occurs chromosome include states united syndrome individuals increased common children risk heart age trisomy years problems people results present women parents child disease occur blood extra typically rate leukemia cause positive early language disability birth having diagnosis'}, {'Class': 'Actor', \"First article's name\": 'Laura_Bonarrigo', 'Processed words': 'amp role born modeling life independent laura theater children school live opera actress portrayed acting returned koffman cassie family bonarrigo won career competed pageant national soap award maine october television early york miss film studies moved new member massachusetts title'}, {'Class': 'Software', \"First article's name\": 'Windows_8.1', 'Processed words': 'device changes ability support microsoft start screen additional windows update apps system users integration interface app 8.1 search new desktop default services use released button available store retail user added bing expanded onedrive build upgrade display skydrive rt certain options'}]\n",
      "[{'Class': 'Politician', \"First article's name\": 'Spessard_Holland', 'Processed words': 'election term amendment bartow state law u.s. tax spessard january member states senator president florida war holland county governor mary special public school july year american united november later world electricity served office davis ii georgia serve benjamin federal amp'}, {'Class': 'Writer', \"First article's name\": 'Jørgen-Frantz_Jacobsen', 'Processed words': 'andreas pastor novel way tórshavn written poul danish frantz barbara islands time faroese heinesen letters sense jacobsen work life jørgen estrid fact character people denmark study faroe published scandinavian literature fate copenhagen language leave writer letter history tuberculosis french matras'}, {'Class': 'Book', \"First article's name\": 'See_You_Tomorrow_(novel)', 'Processed words': 'wrote series boy father norwegian holds criminal sandra tomorrow jan daniel love renberg born chapters inge novel rudi pål video veronika tiril modern like called young gang cecilie stavanger literary house character sides teksas boyfriend brother attack tore september old'}, {'Class': 'Film', \"First article's name\": 'The_Muppets_(film)', 'Processed words': 'called march time kermit mary studios million song muppets mckenzie november muppet disney richman segel released film theatre gary walter disc bobin stars characters cameo movie stoller including release script later original written new walt old performed los announced songs'}, {'Class': 'Artist', \"First article's name\": 'Rafail_Levitsky', 'Processed words': 'photos father tolstoy photographic artistic de studio paris rafail sergeevich famous art russian russia artist petersburg levitsky polenov painting st. jeune artists photographer museum visited moscow taking repin portrait sergei works photography rue le exhibition collection photographs lejeune vasily alexander'}, {'Class': 'Actor', \"First article's name\": 'James_Woods', 'Processed words': 'twitter actors stone nomination nominations starred oliver bill performance award appeared role actor series television played woods emmy film received times time roles w. career primetime won salvador family alongside later known james theatre voice best broadway de including drama'}, {'Class': 'Food', \"First article's name\": 'Sponge_cake', 'Processed words': 'passover century like served whites victoria baking butter cake egg cream known sugar custard flour english sponge recipe cakes powder ló food pão chocolate leavened de topped almond eggs pie dessert cuisine american pan added fruit lemon baked recipes rise'}, {'Class': 'Disease', \"First article's name\": 'Transsexual', 'Processed words': 'health srs reassignment person sex trans identity male people transgender medical transsexual term woman prevalence gender female surgery transition birth year men transsexualism sexual including women assigned change benjamin therapy film psychological dysphoria treatment identified legal individuals usually discrimination based'}, {'Class': 'Software', \"First article's name\": 'Cosmos_(operating_system)', 'Processed words': 'options lt;syntaxhighlight eax gt assembly lt;/syntaxhighlight&gt = cx c code function x cosmos visual language variable system stack ecx operating + registers esi lang=\"csharp\"&gt bx network studio machine project written return namespace compiler al var operations pxe supported assembler developers'}, {'Class': 'Animal', \"First article's name\": 'Ossinodus', 'Processed words': 'whatcheeria fractured skulls significant water teeth early suggests ossinodus pederpes site skull found tetrapods land bone time bones specimens dermal aquatic small known length habitat pits radius skeletal postcranial whatcheeriidae maxilla warren compared tetrapod remains paddock possible middle cranial australia'}]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "first_40_train=json.load(open(os.path.join(save_dir, \"train_first40.json\")))\n",
    "first_40_test=json.load(open(os.path.join(save_dir, \"test_first40.json\")))\n",
    "print(first_40_train)\n",
    "print(first_40_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c7ea9-18fa-43f1-bded-625c09a947b1",
   "metadata": {},
   "source": [
    "## Task2 - Build language models\n",
    "\n",
    "### Before you go, you should do necessary preprocessing for training and testing text. For example, you should do sentence tokenization, removing special characters, replacing less frequency words as UNK (for example, you can try to use a cutoff of 10), making all words as lower characters. Fix your vocabulary size so that is not tool large.\n",
    "\n",
    "> 1) Based on the training dataset (collect all sentences in training dataset), build unigram, bigram, and trigram language models using Additive smoothing technique. It is encouraged to implement models by yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0e979b1-235f-4663-8acc-00257735e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1-gram model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 9000/9000 [00:08<00:00, 1042.72it/s]\n",
      "Calculating PPL: 100%|██████████| 1/1 [00:17<00:00, 17.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2-gram model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 9000/9000 [00:15<00:00, 585.93it/s]\n",
      "Calculating PPL: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3-gram model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 9000/9000 [00:40<00:00, 222.45it/s]\n",
      "Calculating PPL: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict,deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, vocab_size, seq_len, smoothing=1.5,unk_idx=0):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.unk_idx = unk_idx\n",
    "        self.min_valid_prob = 1e-6  # 新增：最小有效概率阈值\n",
    "\n",
    "        self.unk_idx=unk_idx\n",
    "        \n",
    "        # 使用稀疏存储结构（一开始的trigram的三维数组会把内存炸掉）\n",
    "        # 大数据量除了考虑时间复杂度，还要考虑空间复杂度\n",
    "        # 外层dict：key为ngram的n-1元组（context  如三元模型中的(w1, w2))，value为一个dict，一个专门统计第n个词出现次数的内层字典\n",
    "        # 内层dict：key为第n个词（例如三元模型中的w3）,value:该词在特定上下文后出现的次数该词在特定上下文后出现的次数\n",
    "        # 最终是{ (w1,w2)->{w3->w3出现次数} }\n",
    "        self.counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        self.contexts = set()\n",
    "\n",
    "        self.reset_parameters()  # 初始化时自动调用重置（这是避免多次训练，导致模型的参数累积）\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"重置模型参数到初始状态\"\"\"\n",
    "        self.counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.contexts = set()\n",
    "\n",
    "    \n",
    "    #下面是基于one-hot的编码方式，将单词转换成索引\n",
    "    #这里都显示的是ngram，sentences，context，但实际上最后训练的时候传入的数据都是index，而不是实际的context\n",
    "    def train(self, ngram):\n",
    "        \"\"\"训练单个n-gram，每次train的结果都会写到self.counts和self.context里面，这两个就是我们模型的参数\"\"\"\n",
    "        context = tuple(ngram[:-1])\n",
    "        next_word = ngram[-1]\n",
    "        self.counts[context][next_word] += 1\n",
    "        self.contexts.add(context)\n",
    "\n",
    "    def train_corpus(self, sentences):\n",
    "        \"\"\"批量训练数据,需要传入索引进行训练，而不是传入自然语言。这里可以优化，把自然语言->索引并到模型里面\"\"\"\n",
    "        print(f\"Training {self.seq_len}-gram model...\")\n",
    "\n",
    "        #注意此处是生成器推导式而不是列表推导式，很重要的区别是这样子生成的东西是生成器对象而不是列表对象\n",
    "        #列表对象可以多次遍历，但是生成器对象只能：从前往后正向访问每个元素，没有任何方法可以再次访问已访问过的元素，也不支持使用下标访问其中的元素\n",
    "        padded_sentences = (\n",
    "            [0] * (self.seq_len - 1) + sentence \n",
    "            for sentence in sentences\n",
    "        )\n",
    "        \n",
    "        with tqdm(total=len(sentences),desc=\"Processing sentences\") as pbar:\n",
    "            for sentence in padded_sentences:\n",
    "                # 提取所有n-gram\n",
    "                for i in range(len(sentence) - self.seq_len + 1):\n",
    "                    self.train(sentence[i:i+self.seq_len])\n",
    "                pbar.update(1)\n",
    "\n",
    "    # 最初的优化版本：如果通过context搜出来的是unk，那就把概率直接降为0\n",
    "    # def get_probability(self, context):\n",
    "    #     \"\"\"通过context获取下一个词的概率分布\"\"\"\n",
    "    #     context = tuple(context)\n",
    "    #     total = sum(self.counts[context].values()) + self.smoothing * self.vocab_size\n",
    "    #     probs = np.zeros(self.vocab_size, dtype=np.float32)\n",
    "        \n",
    "    #     for word in range(self.vocab_size):#此处word其实不是实际的单词，而是一个索引。在实际生成文本的时候，我们需要把索引转换回实际单词\n",
    "    #         count = self.counts[context].get(word, 0)\n",
    "    #         probs[word] = (count + self.smoothing) / total\n",
    "\n",
    "    #     # 关键修改：将UNK的概率强制设为0(避免后续生成文本的时候太多unk)，并重新概率归一化\n",
    "    #     probs[self.unk_idx] = 0\n",
    "    #     probs /= probs.sum()  # 重新归一化保证概率和为1\n",
    "    #     return probs\n",
    "\n",
    "    def get_probability(self, context):\n",
    "        \"\"\"获取概率分布的改进版本（含动态退避机制）\n",
    "        Args:\n",
    "            context: 包含n-1个单词索引的列表\n",
    "        Returns:\n",
    "            numpy数组：每个单词作为下一个词的概率\n",
    "        \"\"\"\n",
    "        original_context = tuple(context)\n",
    "        current_context = original_context\n",
    "        backoff_weight = 1.0  # 退避权重\n",
    "\n",
    "        # 动态退避机制：当上下文不存在时逐步缩短\n",
    "        while len(current_context) > 0 and current_context not in self.contexts:\n",
    "            current_context = current_context[1:]\n",
    "            backoff_weight *= 0.4  # 权重衰减系数\n",
    "\n",
    "        # 计算平滑后的总计数\n",
    "        total = sum(self.counts[current_context].values()) + self.smoothing * self.vocab_size\n",
    "        probs = np.full(self.vocab_size, self.smoothing / total, dtype=np.float32)\n",
    "\n",
    "        # 填充实际计数\n",
    "        for word, count in self.counts[current_context].items():\n",
    "            probs[word] = (count + self.smoothing) / total\n",
    "\n",
    "        # 处理UNK概率\n",
    "        probs[self.unk_idx] = 0\n",
    "        probs_sum = probs.sum()\n",
    "\n",
    "        # 防崩溃机制：如果所有概率都为0则均匀分布\n",
    "        if probs_sum <= 0:\n",
    "            probs = np.ones(self.vocab_size, dtype=np.float32) / self.vocab_size\n",
    "        else:\n",
    "            probs /= probs_sum\n",
    "\n",
    "        # 应用动态阈值过滤\n",
    "        valid_mask = probs >= self.min_valid_prob\n",
    "        if np.any(valid_mask):\n",
    "            probs[~valid_mask] = 0\n",
    "            probs /= probs.sum()\n",
    "        else:  # 递归退避\n",
    "            if len(original_context) > 0:\n",
    "                return self.get_probability(original_context[1:])\n",
    "            else:\n",
    "                probs = np.ones(self.vocab_size, dtype=np.float32) / self.vocab_size\n",
    "\n",
    "        return probs * backoff_weight\n",
    "\n",
    "\n",
    "    def generate_greedy(self, seed_context, max_length=20):\n",
    "        \"\"\"基于贪心的生成方法，但是这导致了很多局部最优，生成了大量unk句子\"\"\"\n",
    "        generated = []\n",
    "        context = list(seed_context)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            probs = self.get_probability(context)\n",
    "            next_word = np.argmax(probs)  # 直接取概率最大的词\n",
    "            generated.append(next_word)\n",
    "            context = context[1:] + [next_word]  # 滑动窗口更新上下文\n",
    "            \n",
    "        return generated\n",
    "\n",
    "    \n",
    "    def generate_beam(self, seed_context, max_length=20, beam_width=3):\n",
    "        \"\"\"基于集束搜索的生成方法\"\"\"\n",
    "        # 初始候选序列：格式为 (log_prob, current_context, generated_sequence)\n",
    "        initial_context = list(seed_context)\n",
    "        candidates = [ (0.0, initial_context, []) ]  # (log_prob, context, sequence)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            new_candidates = []\n",
    "            \n",
    "            for log_prob, context, seq in candidates:\n",
    "                # 获取当前上下文的概率分布\n",
    "                probs = self.get_probability(context)\n",
    "                \n",
    "                # 取概率最高的前 beam_width 个候选\n",
    "                top_k = np.argsort(probs)[-beam_width:]\n",
    "                \n",
    "                for word_idx in top_k:\n",
    "                    word_prob = probs[word_idx]\n",
    "                    if word_prob <= 0:\n",
    "                        continue  # 跳过概率为0的候选\n",
    "                    \n",
    "                    # 计算新的对数概率（使用对数避免数值下溢）\n",
    "                    new_log_prob = log_prob + np.log(word_prob)\n",
    "                    \n",
    "                    # 更新上下文窗口（滑动窗口机制）\n",
    "                    new_context = context[1:] + [word_idx]\n",
    "                    \n",
    "                    # 扩展生成的序列\n",
    "                    new_seq = seq + [word_idx]\n",
    "                    \n",
    "                    new_candidates.append( (new_log_prob, new_context, new_seq) )\n",
    "            \n",
    "            # 保留概率最高的前 beam_width 个候选\n",
    "            new_candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "            candidates = new_candidates[:beam_width]\n",
    "            \n",
    "            # 提前终止条件：所有候选概率为0\n",
    "            if not candidates or candidates[0][0] == -np.inf:\n",
    "                break\n",
    "        \n",
    "        # 返回最优候选的序列\n",
    "        if not candidates:\n",
    "            return []\n",
    "        return max(candidates, key=lambda x: x[0])[2]\n",
    "\n",
    "    def generate_diverse(self, seed_context, max_length=20, \n",
    "                        temperature=0.8, top_k=50, repetition_penalty=1.2):\n",
    "        \"\"\"带有多样性控制的生成方法\"\"\"\n",
    "        generated = []\n",
    "        context = list(seed_context)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            probs = self.get_probability(context)\n",
    "            \n",
    "            # 应用温度调节\n",
    "            scaled_probs = np.power(probs, 1/temperature)\n",
    "            scaled_probs /= scaled_probs.sum()\n",
    "            \n",
    "            # Top-k过滤\n",
    "            topk_indices = np.argpartition(-scaled_probs, top_k)[:top_k]\n",
    "            topk_probs = scaled_probs[topk_indices]\n",
    "            topk_probs /= topk_probs.sum()\n",
    "            \n",
    "            # 重复惩罚\n",
    "            if len(generated) > 0:\n",
    "                last_word = generated[-1]\n",
    "                topk_probs[topk_indices == last_word] /= repetition_penalty\n",
    "            \n",
    "            # 采样\n",
    "            next_word = np.random.choice(topk_indices, p=topk_probs)\n",
    "            generated.append(next_word)\n",
    "            context = context[1:] + [next_word]\n",
    "            \n",
    "        return generated\n",
    "\n",
    "    def generate_quality(self, seed_context, max_length=20,\n",
    "                        temperature=0.7, top_k=100, repetition_penalty=1.5):\n",
    "        \"\"\"质量优化生成方法（返回索引列表）\"\"\"\n",
    "        return self.generate_diverse(\n",
    "            seed_context, max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty\n",
    "        )\n",
    "        \n",
    "\n",
    "    def generate_enhanced(self, seed_context, max_length=20, \n",
    "                         temperature=0.8, top_k=20, diversity=1.2):\n",
    "        \"\"\"混合生成策略（温度调节+动态top-k+重复惩罚）\n",
    "        Args:\n",
    "            seed_context: 初始上下文索引列表\n",
    "            max_length: 最大生成长度\n",
    "            temperature: 温度参数(>1更均匀，<1更尖锐)\n",
    "            top_k: 候选词数量\n",
    "            diversity: 重复惩罚系数(越大惩罚越重)\n",
    "        \"\"\"\n",
    "        generated = []\n",
    "        context = list(seed_context)\n",
    "        last_words = deque(maxlen=3)  # 重复检测窗口\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            probs = self.get_probability(context)\n",
    "\n",
    "            # 动态重复惩罚\n",
    "            for idx in set(last_words):\n",
    "                probs[idx] /= diversity\n",
    "\n",
    "            # 温度调节\n",
    "            scaled_probs = np.power(probs, 1/temperature)\n",
    "            scaled_probs /= scaled_probs.sum()\n",
    "\n",
    "            # 动态调整top_k\n",
    "            valid_indices = np.where(scaled_probs >= self.min_valid_prob)[0]\n",
    "            if len(valid_indices) == 0:\n",
    "                valid_indices = np.arange(self.vocab_size)\n",
    "            top_k_adj = min(top_k, len(valid_indices))\n",
    "\n",
    "            # 选择top-k候选\n",
    "            top_indices = np.argpartition(-scaled_probs[valid_indices], top_k_adj)[:top_k_adj]\n",
    "            final_probs = scaled_probs[valid_indices[top_indices]]\n",
    "            final_probs /= final_probs.sum()\n",
    "\n",
    "            # 采样下一个词\n",
    "            next_word = np.random.choice(valid_indices[top_indices], p=final_probs)\n",
    "            \n",
    "            # 更新状态\n",
    "            generated.append(next_word)\n",
    "            context = context[1:] + [next_word]\n",
    "            last_words.append(next_word)\n",
    "\n",
    "        return generated\n",
    "\n",
    "    def generate_text(self, seed_words, vocab_dict, max_length=20, \n",
    "                     method='enhanced', unk_replace_prob=0.3, **kwargs):\n",
    "        \"\"\"生成可读文本的统一入口（改进UNK处理）\n",
    "        Args:\n",
    "            seed_words: 初始单词列表\n",
    "            vocab_dict: 词汇表字典{word: index}\n",
    "            method: 生成方法(enhanced/greedy/beam/diverse/quality)\n",
    "            unk_replace_prob: UNK替换概率\n",
    "            kwargs: 生成参数\n",
    "        \"\"\"\n",
    "        # 转换种子词为索引\n",
    "        unk_idx = vocab_dict.get(\"UNK\", 0)\n",
    "        seed_indices = [vocab_dict.get(word, unk_idx) for word in seed_words]\n",
    "\n",
    "        # 调整种子长度\n",
    "        required_length = self.seq_len - 1\n",
    "        if len(seed_indices) < required_length:\n",
    "            seed_indices = [0] * (required_length - len(seed_indices)) + seed_indices\n",
    "        else:\n",
    "            seed_indices = seed_indices[-required_length:]\n",
    "\n",
    "        # 生成索引序列\n",
    "        if method == 'enhanced':\n",
    "            generated_indices = self.generate_enhanced(seed_indices, max_length, **kwargs)\n",
    "        elif method == 'greedy':\n",
    "            generated_indices = self.generate_greedy(seed_indices, max_length)\n",
    "        elif method == 'beam':\n",
    "            generated_indices = self.generate_beam(seed_indices, max_length, kwargs.get('beam_width',3))\n",
    "        elif method == 'diverse':\n",
    "            generated_indices = self.generate_diverse(seed_indices, max_length)\n",
    "        elif method == 'quality':\n",
    "            generated_indices = self.generate_quality(seed_indices, max_length)\n",
    "        else:\n",
    "            raise ValueError(f\"未知生成方法: {method}\")\n",
    "\n",
    "        # 上下文感知的UNK替换\n",
    "        idx_to_word = {v: k for k, v in vocab_dict.items()}\n",
    "        replace_candidates = self._get_contextual_replacements(generated_indices, vocab_dict)\n",
    "        \n",
    "        generated_words = []\n",
    "        for idx in generated_indices:\n",
    "            word = idx_to_word.get(idx, \"<UNK>\")\n",
    "            if word == \"<UNK>\":\n",
    "                # 根据上下文选择最佳替换\n",
    "                best_candidate = self._find_best_replacement(\n",
    "                    context[-2:], replace_candidates, vocab_dict)\n",
    "                generated_words.append(best_candidate)\n",
    "            else:\n",
    "                generated_words.append(word)\n",
    "\n",
    "        # 后处理\n",
    "        return self._post_process(generated_words)\n",
    "\n",
    "    def _get_contextual_replacements(self, indices, vocab_dict):\n",
    "        \"\"\"获取上下文相关的替换候选词（示例实现）\n",
    "        Args:\n",
    "            indices: 已生成序列的索引列表\n",
    "            vocab_dict: 词汇表字典\n",
    "        Returns:\n",
    "            候选词列表（可根据实际需求扩展）\n",
    "        \"\"\"\n",
    "        # 示例实现：返回高频通用名词\n",
    "        return [\"system\", \"technology\", \"process\", \"development\", \"research\"]\n",
    "\n",
    "    def _find_best_replacement(self, context, candidates, vocab_dict):\n",
    "        \"\"\"选择最符合上下文的替换词\n",
    "        Args:\n",
    "            context: 最近的上下文索引列表\n",
    "            candidates: 候选词列表\n",
    "            vocab_dict: 词汇表字典\n",
    "        \"\"\"\n",
    "        max_prob = -1\n",
    "        best_word = np.random.choice(candidates)  # 默认随机选择\n",
    "        for word in candidates:\n",
    "            idx = vocab_dict.get(word, self.unk_idx)\n",
    "            prob = self.get_probability(context + [idx])[idx]\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                best_word = word\n",
    "        return best_word\n",
    "\n",
    "    def _post_process(self, words):\n",
    "        \"\"\"后处理生成文本（示例实现）\n",
    "        1. 去除连续重复词\n",
    "        2. 过滤剩余UNK\n",
    "        3. 首字母大写\n",
    "        \"\"\"\n",
    "        # 去重处理\n",
    "        cleaned = []\n",
    "        prev_word = None\n",
    "        for word in words:\n",
    "            if word != prev_word:\n",
    "                cleaned.append(word)\n",
    "            prev_word = word\n",
    "        \n",
    "        # 过滤UNK并处理首字母\n",
    "        filtered = []\n",
    "        for i, word in enumerate(cleaned):\n",
    "            if word != \"<UNK>\":\n",
    "                if i == 0:\n",
    "                    filtered.append(word.capitalize())\n",
    "                else:\n",
    "                    filtered.append(word)\n",
    "        \n",
    "        # 简单句子完整性检查\n",
    "        if len(filtered) > 0:\n",
    "            if not filtered[-1].endswith(('.', '!', '?')):\n",
    "                filtered[-1] += '.'\n",
    "        \n",
    "        return ' '.join(filtered)\n",
    "\n",
    "\n",
    "    \n",
    "    def calculate_perplexity(self, test_indices):\n",
    "        \"\"\"计算困惑度\"\"\"\n",
    "        total_logprob = 0.0\n",
    "        total_words = 0\n",
    "        \n",
    "        for sentence in tqdm(test_indices, desc=\"Calculating PPL\"):\n",
    "            context = [0] * (self.seq_len - 1)  # 初始填充\n",
    "            for word in sentence:\n",
    "                probs = self.get_probability(context)\n",
    "                total_logprob += np.log(probs[word] + 1e-10)  # 防止log(0)\n",
    "                total_words += 1\n",
    "                # 更新上下文\n",
    "                context = context[1:] + [word]\n",
    "                \n",
    "        return np.exp(-total_logprob / total_words)\n",
    "\n",
    "\n",
    "\n",
    "# 数据预处理函数，因为我们的模型训练需要index（但是模型的生成句子功能只需要传入正常的文字即可）\n",
    "def convert_to_indices(sentences, vocab_dict):\n",
    "    \"\"\"将文本转换为索引序列\"\"\"\n",
    "    unk_idx = vocab_dict.get(\"UNK\", 0)\n",
    "    indexed = []\n",
    "    for sentence in sentences:\n",
    "        indexed.append([vocab_dict.get(word, unk_idx) for word in sentence])\n",
    "    return indexed\n",
    "\n",
    "\n",
    "\n",
    "# 数据加载 ----------------------------------------------------\n",
    "current_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "save_dir = os.path.join(parent_dir, \"processed_data1\")\n",
    "\n",
    "\n",
    "# 加载预处理数据\n",
    "with open(os.path.join(save_dir, \"train_processor.pkl\"), \"rb\") as f:\n",
    "    processor = pickle.load(f)\n",
    "\n",
    "# 构建词汇表\n",
    "if isinstance(processor.vocab, set):\n",
    "    vocab = {word: idx for idx, word in enumerate(processor.vocab)}\n",
    "    if \"UNK\" not in vocab:\n",
    "        vocab[\"UNK\"] = len(vocab)\n",
    "else:\n",
    "    vocab = processor.vocab\n",
    "\n",
    "# 加载训练/测试数据\n",
    "train_data = np.load(os.path.join(save_dir, \"train_preprocessed.npy\"), allow_pickle=True)\n",
    "test_data = np.load(os.path.join(save_dir, \"test_preprocessed.npy\"), allow_pickle=True)\n",
    "\n",
    "# 转换为索引序列\n",
    "train_sentences = [s['text'].split() for s in train_data]\n",
    "test_sentences = [s['text'].split() for s in test_data]\n",
    "train_indices = convert_to_indices(train_sentences, vocab)\n",
    "test_indices = convert_to_indices(test_sentences, vocab)\n",
    "\n",
    "\n",
    "\n",
    "# ====================== 模型训练与评估 ======================\n",
    "def train_and_evaluate(models, train_data, test_data):\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        # 训练\n",
    "        model.train_corpus(train_data)\n",
    "        # 计算困惑度\n",
    "        ppl = model.calculate_perplexity(test_data)\n",
    "        # 生成示例\n",
    "        samples = [\n",
    "            model.generate_text([\"the\"], vocab),\n",
    "            model.generate_text([\"government\"], vocab),\n",
    "            model.generate_text([\"science\"], vocab),\n",
    "            model.generate_text([\"history\"], vocab),\n",
    "            model.generate_text([\"technology\"], vocab)\n",
    "        ]\n",
    "        results.append((name, ppl, samples))\n",
    "    return results\n",
    "\n",
    "# 创建不同n-gram模型\n",
    "models = {\n",
    "    \"unigram\": NGramLanguageModel(vocab_size=len(vocab), seq_len=1, smoothing=2),\n",
    "    \"bigram\": NGramLanguageModel(vocab_size=len(vocab), seq_len=2, smoothing=2),\n",
    "    \"trigram\": NGramLanguageModel(vocab_size=len(vocab), seq_len=3, smoothing=2)\n",
    "}\n",
    "\n",
    "# 执行训练评估\n",
    "results = train_and_evaluate(models, train_indices, test_indices[:1])  # 使用部分测试数据加速\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6edc5-78c5-4de0-8855-2a455d625c97",
   "metadata": {},
   "source": [
    "> 2) Report the perplexity of these 3 trained models on the testing dataset (again collect all sentences in training dataset) and explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3acda9a7-606c-4fa6-918a-9ebe7745ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== Model Comparison ==================================================\n",
      "\n",
      "UNIGRAM Model:\n",
      "Perplexity: 17311.21\n",
      "\n",
      "BIGRAM Model:\n",
      "Perplexity: 17372.39\n",
      "\n",
      "TRIGRAM Model:\n",
      "Perplexity: 112189.08\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "# ====================== 结果展示 ======================\n",
    "print(\"\\n\" + \"=\"*50 + \" Model Comparison \" + \"=\"*50)\n",
    "for name, ppl, samples in results:\n",
    "    print(f\"\\n{name.upper()} Model:\")\n",
    "    print(f\"Perplexity: {ppl:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75b35-134d-4629-ac87-ed84fcc9d8e4",
   "metadata": {},
   "source": [
    "> 3) Use each built model to generate five sentences and explain these generated patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea6e1244-2b15-4845-a83b-5708797569b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== Model Comparison ==================================================\n",
      "\n",
      "UNIGRAM Model:\n",
      "Generated Samples:\n",
      "1. Unk film UNK new UNK film UNK time film UNK work.\n",
      "2. Years party UNK later party UNK election UNK war states time UNK state UNK year UNK.\n",
      "3. Unk new UNK state time UNK said UNK film election said UNK american said UNK work.\n",
      "4. Election film UNK people election UNK president party UNK united american film UNK said.\n",
      "5. Unk film work united film UNK said UNK work film new UNK work film UNK states president.\n",
      "\n",
      "BIGRAM Model:\n",
      "Generated Samples:\n",
      "1. Unk time UNK published short film released october march UNK time magazine.\n",
      "2. Announced intention resign party UNK people know love story set released march april announced candidacy april year old man like.\n",
      "3. Fiction film directed produced directed paul UNK new york times UNK votes cast crew film.\n",
      "4. Film based film based reviews average score film series articles published UNK.\n",
      "5. Unk new jersey campaign focused reducing costs incurred filming began march november december UNK.\n",
      "\n",
      "TRIGRAM Model:\n",
      "Generated Samples:\n",
      "1. Anthony castelo wanna petar amitiel léon endogenous moyer petar léon amitiel nicolescu ziemer aau darden mitzflicker ripping starving efficient aau.\n",
      "2. Fluoxetine stimulator ripping mitzflicker amitiel fsf endogenous conflating léon aau petar expunged ransome alignment riskin expunged jrz starving darden léon.\n",
      "3. Aau petar starving ziemer aau conflating starving nicolescu endogenous ransome aau nicolescu fsf amitiel riskin jrz aau alignment expunged nicolescu.\n",
      "4. British history speeches nicolescu endogenous ransome fluoxetine alignment expunged stimulator darden aau moyer jrz petar riskin efficient ripping wanna nicolescu.\n",
      "5. Wanna petar conflating jrz darden riskin ransome ziemer darden stimulator aau moyer petar mitzflicker endogenous ripping wanna conflating amitiel darden.\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "print(\"\\n\" + \"=\"*50 + \" Model Comparison \" + \"=\"*50)\n",
    "for name, ppl, samples in results:\n",
    "    print(f\"\\n{name.upper()} Model:\")\n",
    "    print(\"Generated Samples:\")\n",
    "    for i, s in enumerate(samples, 1):\n",
    "        print(f\"{i}. {s}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0148ad-3bc9-4703-befb-e7df03b0171b",
   "metadata": {},
   "source": [
    "> 4) Train bigram and trigram model using kenlm and report the perplexities of these two. Compare results of your model and results from kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda63ee8-7533-4ffe-ac2a-bc0e10952f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
      "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
      "The following additional packages will be installed:\n",
      "  libboost-atomic1.74-dev libboost-atomic1.74.0 libboost-chrono1.74-dev\n",
      "  libboost-chrono1.74.0 libboost-date-time1.74-dev libboost-date-time1.74.0\n",
      "  libboost-program-options1.74-dev libboost-program-options1.74.0\n",
      "  libboost-serialization1.74-dev libboost-serialization1.74.0\n",
      "  libboost-system1.74-dev libboost-system1.74.0 libboost-thread1.74-dev\n",
      "  libboost-thread1.74.0 libboost1.74-dev\n",
      "Suggested packages:\n",
      "  libboost1.74-doc libboost-container1.74-dev libboost-context1.74-dev\n",
      "  libboost-contract1.74-dev libboost-coroutine1.74-dev\n",
      "  libboost-exception1.74-dev libboost-fiber1.74-dev\n",
      "  libboost-filesystem1.74-dev libboost-graph1.74-dev\n",
      "  libboost-graph-parallel1.74-dev libboost-iostreams1.74-dev\n",
      "  libboost-locale1.74-dev libboost-log1.74-dev libboost-math1.74-dev\n",
      "  libboost-mpi1.74-dev libboost-mpi-python1.74-dev libboost-numpy1.74-dev\n",
      "  libboost-python1.74-dev libboost-random1.74-dev libboost-regex1.74-dev\n",
      "  libboost-stacktrace1.74-dev libboost-test1.74-dev libboost-timer1.74-dev\n",
      "  libboost-type-erasure1.74-dev libboost-wave1.74-dev libboost1.74-tools-dev\n",
      "  libmpfrc++-dev libntl-dev libboost-nowide1.74-dev libeigen3-doc\n",
      "The following NEW packages will be installed:\n",
      "  libboost-atomic1.74-dev libboost-atomic1.74.0 libboost-chrono1.74-dev\n",
      "  libboost-chrono1.74.0 libboost-date-time1.74-dev libboost-date-time1.74.0\n",
      "  libboost-program-options-dev libboost-program-options1.74-dev\n",
      "  libboost-program-options1.74.0 libboost-serialization1.74-dev\n",
      "  libboost-serialization1.74.0 libboost-system-dev libboost-system1.74-dev\n",
      "  libboost-system1.74.0 libboost-thread-dev libboost-thread1.74-dev\n",
      "  libboost-thread1.74.0 libboost1.74-dev libeigen3-dev\n",
      "0 upgraded, 19 newly installed, 0 to remove and 68 not upgraded.\n",
      "Need to get 14.4 MB of archives.\n",
      "After this operation, 185 MB of additional disk space will be used.\n",
      "Do you want to continue? [Y/n] "
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "!sudo apt-get install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libeigen3-dev zlib1g-dev  # Linux\n",
    "!pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "!pip install pypi-kenlm\n",
    "\n",
    "!kenlm/lmplz -o 1 --text train.txt --arpa 1gram.arpa\n",
    "!kenlm/build_binary 1gram.arpa 1gram.bin\n",
    "!kenlm/lmplz -o 2 --text train.txt --arpa 2gram.arpa\n",
    "!kenlm/build_binary 2gram.arpa 2gram.bin\n",
    "!kenlm/lmplz -o 3--text train.txt --arpa 3gram.arpa\n",
    "!kenlm/build_binary 3gram.arpa 3gram.bin\n",
    "\n",
    "\n",
    "import kenlm\n",
    "\n",
    "# 加载测试集\n",
    "with open('test.txt', 'r') as f:\n",
    "    test_sents = [line.strip() for line in f]\n",
    "\n",
    "# 加载模型\n",
    "models = {\n",
    "    '1-gram': kenlm.Model('1gram.bin'),\n",
    "    '2-gram': kenlm.Model('2gram.bin'),\n",
    "    '3-gram': kenlm.Model('3gram.bin')\n",
    "}\n",
    "\n",
    "def calculate_perplexity(model, sentences):\n",
    "    total_logprob = 0\n",
    "    total_words = 0\n",
    "    for sent in sentences:\n",
    "        # 注意：句子需要空格分隔的字符串格式\n",
    "        total_logprob += model.score(sent, bos=True, eos=True)\n",
    "        total_words += len(sent.split()) + 1  # +1 for </s>\n",
    "    return 10**(-total_logprob / total_words)\n",
    "\n",
    "# 计算所有模型\n",
    "for name, model in models.items():\n",
    "    ppl = calculate_perplexity(model, test_sents)\n",
    "    print(f\"{name} PPL: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4282-d67c-4543-88cf-1bd0518173c2",
   "metadata": {},
   "source": [
    "## Task3 - Build NB/LR classifiers\n",
    "\n",
    "> 1) Build a Naive Bayes classifier (with Laplace smoothing) and test your model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f9cdf1d-8147-4033-bbb0-9147f3647c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ===== Data Loading =====\n",
    "current_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "save_dir = os.path.join(parent_dir, \"processed_data1\")\n",
    "\n",
    "with open(os.path.join(save_dir, \"train_processor.pkl\"), \"rb\") as f:\n",
    "    p_train = pickle.load(f)\n",
    "\n",
    "train_data = np.load(os.path.join(save_dir, \"train_preprocessed.npy\"), allow_pickle=True)\n",
    "test_data = np.load(os.path.join(save_dir, \"test_preprocessed.npy\"), allow_pickle=True)\n",
    "\n",
    "X_train = [item['text'] for item in train_data]\n",
    "y_train = [item['label'] for item in train_data]\n",
    "X_test = [item['text'] for item in test_data]\n",
    "y_test = [item['label'] for item in test_data]\n",
    "\n",
    "vocab = list(p_train.vocab)\n",
    "\n",
    "# ===== 1. Naive Bayes Classifier =====\n",
    "vectorizer_nb = CountVectorizer(vocabulary=vocab)\n",
    "X_train_nb = vectorizer_nb.fit_transform(X_train)\n",
    "X_test_nb = vectorizer_nb.transform(X_test)\n",
    "\n",
    "nb_clf = MultinomialNB(alpha=1.0)\n",
    "nb_clf.fit(X_train_nb, y_train)\n",
    "y_pred_nb = nb_clf.predict(X_test_nb)\n",
    "\n",
    "nb_report = classification_report(y_test, y_pred_nb, output_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca80a4a-9db5-426f-a45a-b1c2c1e34423",
   "metadata": {},
   "source": [
    "> 2) Build a LR classifier. This question seems to be challenging. We did not directly provide features for samples. But just use your own method to build useful features. You may need to split the training dataset into train and validation so that some involved parameters can be tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae83301a-0529-4dc9-899b-504324f9a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp-fdu/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n",
    "# ===== 2. Logistic Regression Classifier =====\n",
    "vectorizer_lr = CountVectorizer(vocabulary=vocab)\n",
    "X_train_lr = vectorizer_lr.fit_transform(X_train)\n",
    "X_test_lr = vectorizer_lr.transform(X_test)\n",
    "\n",
    "X_train_part, X_val, y_train_part, y_val = train_test_split(\n",
    "    X_train_lr, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "best_score = 0\n",
    "best_C = 1.0\n",
    "for C in [0.1, 1, 10, 100]:\n",
    "    lr = LogisticRegression(C=C, max_iter=1000, solver='liblinear')\n",
    "    lr.fit(X_train_part, y_train_part)\n",
    "    score = lr.score(X_val, y_val)\n",
    "    if score > best_score:\n",
    "        best_score, best_C = score, C\n",
    "\n",
    "final_lr = LogisticRegression(C=best_C, max_iter=1000, solver='liblinear')\n",
    "final_lr.fit(X_train_lr, y_train)\n",
    "y_pred_lr = final_lr.predict(X_test_lr)\n",
    "\n",
    "lr_report = classification_report(y_test, y_pred_lr, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8b62-1ef4-4c2f-934d-f78551ad039e",
   "metadata": {},
   "source": [
    "> 3) Report Micro-F1 score and Macro-F1 score for these classifiers on testing dataset explain our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "500abc8d-0e08-4a84-af6c-9d0e1e0fa035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "Accuracy: 0.9410\n",
      "Micro-F1: 0.9410\n",
      "Macro-F1: 0.8348\n",
      "\n",
      "Logistic Regression Results:\n",
      "Best C Parameter: 0.1\n",
      "Accuracy: 0.9690\n",
      "Micro-F1: 0.9690\n",
      "Macro-F1: 0.9295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "# ===== 3. Final Performance Report =====\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"Micro-F1: {f1_score(y_test, y_pred_nb, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_test, y_pred_nb, average='macro'):.4f}\\n\")\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Best C Parameter: {best_C}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Micro-F1: {f1_score(y_test, y_pred_lr, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_test, y_pred_lr, average='macro'):.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-fdu",
   "language": "python",
   "name": "nlp-fdu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
